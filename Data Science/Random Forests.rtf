{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Regression Trees\
	A form of classification that uses a decision tree to choose output\
	Can split regions on any number of x\
	Learning step functions to model the data\
\
Learning Tree\
	Common way is using a greedy algorithm\
		Start with one node, split to 2 children\
		Split in a way that reduces uncertainty (minimize squared error) the most, repeat\
		Check every split, choose the one with the lowest sum of squared errors\
		Grow tree to many branches and then use an algorithm to prune back\
\
Bootstrap\
	Way of testing confidence of a Statistic for a data set\
	Calculate by:\
		Take n (n is data set size) random samples from data (can repeat data points)\
		Calculate Statistic based off that sample\
		Repeat b times and use the sample Statistics to calculate variance and mean\
\
Bagging\
	Use the Learning tree algorithm (Above) to create a tree\
	Bootstrap the data and create another tree \
	Benefits cap around 25 bootstraps\
	Benefits stop because bootstrap samples are correlated\
\
Random Forest\
	Modification of bagging where trees are designed to reduce correlation\
	Still bootstrap, but split region, only consider a random subset,m, of d dimensions of x\
	Make the best split based on that random subset\
	}